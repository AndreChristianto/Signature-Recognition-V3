{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train, Test, and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_path =  \"../feature/BENGALI-feature/train_features.npy\"\n",
    "train_names_path = \"../feature/BENGALI-feature/train_names.npy\"\n",
    "train_labels_path = \"../feature/BENGALI-feature/train_labels.npy\"\n",
    "\n",
    "test_features_path = \"../feature/BENGALI-feature/test_features.npy\"\n",
    "test_names_path = \"../feature/BENGALI-feature/test_names.npy\"\n",
    "test_labels_path = \"../feature/BENGALI-feature/test_labels.npy\"\n",
    "\n",
    "validation_features_path = \"../feature/BENGALI-feature/validation_features.npy\"\n",
    "validation_names_path = \"../feature/BENGALI-feature/validation_names.npy\"\n",
    "validation_labels_path = \"../feature/BENGALI-feature/validation_labels.npy\"\n",
    "\n",
    "gallery_feature_path = \"../feature/BENGALI-feature/gallery_features.npy\"\n",
    "gallery_name_path = \"../feature/BENGALI-feature/gallery_names.npy\"\n",
    "\n",
    "train_features = np.load(train_features_path)\n",
    "train_names = np.load(train_names_path)\n",
    "train_labels = np.load(train_labels_path)\n",
    "\n",
    "test_features = np.load(test_features_path)\n",
    "test_names = np.load(test_names_path)\n",
    "test_labels = np.load(test_labels_path)\n",
    "\n",
    "validation_features = np.load(validation_features_path)\n",
    "validation_names = np.load(validation_names_path)\n",
    "validation_labels = np.load(validation_labels_path)\n",
    "\n",
    "gallery_features = np.load(gallery_feature_path)\n",
    "gallery_names = np.load(gallery_name_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 512)\n",
      "(3200,)\n",
      "(3200,)\n",
      "(1100, 512)\n",
      "(1100,)\n",
      "(1100,)\n",
      "(1100, 512)\n",
      "(1100,)\n",
      "(1100,)\n",
      "(100, 512)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_names.shape)\n",
    "print(test_features.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_names.shape)\n",
    "print(validation_features.shape)\n",
    "print(validation_labels.shape)\n",
    "print(validation_names.shape)\n",
    "print(gallery_features.shape)\n",
    "print(gallery_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = set(train_names.flatten())\n",
    "sorted_names = sorted(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100']\n"
     ]
    }
   ],
   "source": [
    "print(sorted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001' '002' '003' '004' '005' '006' '007' '008' '009' '010' '011' '012'\n",
      " '013' '014' '015' '016' '017' '018' '019' '020' '021' '022' '023' '024'\n",
      " '025' '026' '027' '028' '029' '030' '031' '032' '033' '034' '035' '036'\n",
      " '037' '038' '039' '040' '041' '042' '043' '044' '045' '046' '047' '048'\n",
      " '049' '050' '051' '052' '053' '054' '055' '056' '057' '058' '059' '060'\n",
      " '061' '062' '063' '064' '065' '066' '067' '068' '069' '070' '071' '072'\n",
      " '073' '074' '075' '076' '077' '078' '079' '080' '081' '082' '083' '084'\n",
      " '085' '086' '087' '088' '089' '090' '091' '092' '093' '094' '095' '096'\n",
      " '097' '098' '099' '100']\n"
     ]
    }
   ],
   "source": [
    "print(gallery_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(unique_names, gallery_features, features, labels):\n",
    "    anchors, positives, negatives = [], [], []\n",
    "    for name, gallery in zip(unique_names, gallery_features):\n",
    "        # find all the instances for name\n",
    "        indices = np.where(train_names == name)[0]\n",
    "        # print(indices)\n",
    "        \n",
    "        # separate instances to positive and negative\n",
    "        positive_set, negative_set = [], []\n",
    "        for index in indices:\n",
    "            # print(index)\n",
    "            if labels[index] == 0:\n",
    "                negative_set.append(features[index])\n",
    "                # print(features[index])\n",
    "            else:\n",
    "                positive_set.append(features[index]) \n",
    "\n",
    "        negative_set = np.array(negative_set)\n",
    "        positive_set = np.array(positive_set)\n",
    "\n",
    "        # use feature gallery for anchor\n",
    "        # print(gallery.size)\n",
    "        anchor = gallery\n",
    "        \n",
    "        # print(positive_set.size)\n",
    "        # print(negative_set.size)\n",
    "\n",
    "        # randomize data picks to accomodate for unequal data size (total size is based on negative data size due to it being bigger than positive data size)\n",
    "        positive_idx = np.random.choice(len(positive_set), size=len(negative_set), replace=True)\n",
    "        positive = positive_set[positive_idx]\n",
    "        \n",
    "        anchor_set = []\n",
    "        for _ in range(0, len(negative_set)):\n",
    "            anchor_set.append(anchor)\n",
    "\n",
    "        anchors.append(anchor_set)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative_set)\n",
    "        # print(len(anchors))\n",
    "        # print(len(positives))\n",
    "        # print(len(negatives))\n",
    "\n",
    "    anchors = np.concatenate(anchors, axis=0)\n",
    "    \n",
    "    positives = np.concatenate(positives, axis=0)\n",
    "\n",
    "    negatives = np.concatenate(negatives, axis=0)\n",
    "    \n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, positives, negatives = create_triplets(sorted_names, gallery_features, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors shape: (1800, 512)\n",
      "Positives shape: (1800, 512)\n",
      "Negatives shape: (1800, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Anchors shape:\", anchors.shape)\n",
    "print(\"Positives shape:\", positives.shape)\n",
    "print(\"Negatives shape:\", negatives.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andreas Christianto\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (train_features.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andreas Christianto\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet_loss_model(embedding_model, input_shape):\n",
    "    anchor_input = layers.Input(shape=input_shape, name='anchor_input')\n",
    "    positive_input = layers.Input(shape=input_shape, name='positive_input')\n",
    "    negative_input = layers.Input(shape=input_shape, name='negative_input')\n",
    "\n",
    "    # Get embeddings\n",
    "    anchor_embedding = embedding_model(anchor_input)\n",
    "    positive_embedding = embedding_model(positive_input)\n",
    "    negative_embedding = embedding_model(negative_input)\n",
    "\n",
    "    # Triplet loss function\n",
    "    margin = 0.2\n",
    "    positive_distance = tf.reduce_sum(tf.square(anchor_embedding - positive_embedding), axis=1)\n",
    "    negative_distance = tf.reduce_sum(tf.square(anchor_embedding - negative_embedding), axis=1)\n",
    "    loss = tf.maximum(0.0, positive_distance - negative_distance + margin)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    triplet_loss_model = models.Model(inputs=[anchor_input, positive_input, negative_input], outputs=loss)\n",
    "    return triplet_loss_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 427009 (1.63 MB)\n",
      "Trainable params: 427009 (1.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andreas Christianto\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triplet_model = create_triplet_loss_model(embedding_model, input_shape)\n",
    "triplet_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " anchor_input (InputLayer)   [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " positive_input (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " negative_input (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 1)                    427009    ['anchor_input[0][0]',        \n",
      "                                                                     'positive_input[0][0]',      \n",
      "                                                                     'negative_input[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLamb  (None, 1)                    0         ['sequential[0][0]',          \n",
      " da)                                                                 'sequential[1][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLa  (None, 1)                    0         ['sequential[0][0]',          \n",
      " mbda)                                                               'sequential[2][0]']          \n",
      "                                                                                                  \n",
      " tf.math.square (TFOpLambda  (None, 1)                    0         ['tf.math.subtract[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.square_1 (TFOpLamb  (None, 1)                    0         ['tf.math.subtract_1[0][0]']  \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLa  (None,)                      0         ['tf.math.square[0][0]']      \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOp  (None,)                      0         ['tf.math.square_1[0][0]']    \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLa  (None,)                      0         ['tf.math.reduce_sum[0][0]',  \n",
      " mbda)                                                               'tf.math.reduce_sum_1[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None,)                      0         ['tf.math.subtract_2[0][0]']  \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.maximum (TFOpLambd  (None,)                      0         ['tf.__operators__.add[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpL  ()                           0         ['tf.math.maximum[0][0]']     \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 427009 (1.63 MB)\n",
      "Trainable params: 427009 (1.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "triplet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Andreas Christianto\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "57/57 [==============================] - 3s 8ms/step - loss: 0.0126\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0047\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0021\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 9.9096e-04\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0027\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 6.3359e-04\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 5.3216e-04\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 3.1062e-04\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 2.4155e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ab90b2c990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_model.fit([anchors, positives, negatives], np.zeros_like(anchors), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = embedding_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.17454545454545456\n"
     ]
    }
   ],
   "source": [
    "binary_predictions = (predictions >= 0.5).astype(int)\n",
    "accuracy = accuracy_score(test_labels, binary_predictions)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
